import logging
import os
import tempfile
from typing import List

from telegram import Message, Update
from telegram.ext import CallbackContext

from config import ALLOWED_USER_IDS
from services.gemini_service import analyze_content
from services.notion_service.utils import extract_hashtags, remove_hashtags_from_text, merge_tags
from services.notion_service import upload_image_to_notion
from utils.helpers import is_url_only
from utils.text_formatter import (
    extract_urls_from_entities,
    parse_message_entities,
)

from .pdf_handlers import handle_pdf_document
from .test_handlers import handle_test_message
from .todo_handlers import handle_todo_message

# å¯¼å…¥å¤„ç†å™¨
from .url_handlers import handle_multiple_urls_message, handle_url_message

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


def download_and_upload_photos(message, context) -> list:
    """
    ä¸‹è½½ Telegram æ¶ˆæ¯ä¸­çš„å›¾ç‰‡å¹¶ä¸Šä¼ åˆ° Notion

    å‚æ•°ï¼š
        message: Telegram æ¶ˆæ¯å¯¹è±¡
        context: Telegram ä¸Šä¸‹æ–‡å¯¹è±¡

    è¿”å›ï¼š
        list: æˆåŠŸä¸Šä¼ çš„ file_upload_id åˆ—è¡¨
    """
    file_upload_ids = []

    if not message.photo:
        return file_upload_ids

    # Telegram è¿”å›å¤šä¸ªåˆ†è¾¨ç‡çš„å›¾ç‰‡ï¼Œæœ€åä¸€ä¸ªæ˜¯æœ€é«˜åˆ†è¾¨ç‡
    photo = message.photo[-1]

    try:
        # ä¸‹è½½å›¾ç‰‡
        file = context.bot.get_file(photo.file_id)
        temp_path = os.path.join(tempfile.gettempdir(), f"{photo.file_unique_id}.jpg")
        file.download(temp_path)

        try:
            # ä¸Šä¼ å›¾ç‰‡åˆ° Notion
            logger.info(f"å¼€å§‹ä¸Šä¼ å›¾ç‰‡åˆ° Notion: {temp_path}")
            file_upload_id = upload_image_to_notion(file_path=temp_path)

            if file_upload_id:
                file_upload_ids.append(file_upload_id)
                logger.info(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œfile_upload_id: {file_upload_id}")
            else:
                logger.error("å›¾ç‰‡ä¸Šä¼ åˆ° Notion å¤±è´¥")

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_path)
            except OSError as file_error:
                logger.warning(f"æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{file_error}")

    except Exception as e:
        logger.error(f"ä¸‹è½½æˆ–ä¸Šä¼ å›¾ç‰‡æ—¶å‡ºé”™ï¼š{e}")

    return file_upload_ids


def download_and_upload_photos_batch(messages: List[Message], context: CallbackContext) -> list:
    """
    æ‰¹é‡ä¸‹è½½å¤šæ¡æ¶ˆæ¯ä¸­çš„å›¾ç‰‡å¹¶ä¸Šä¼ åˆ° Notion

    å‚æ•°ï¼š
        messages: Telegram æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ï¼ˆæ¥è‡ªåŒä¸€ media groupï¼‰
        context: Telegram ä¸Šä¸‹æ–‡å¯¹è±¡

    è¿”å›ï¼š
        list: æˆåŠŸä¸Šä¼ çš„ file_upload_id åˆ—è¡¨ï¼Œé¡ºåºä¸æ¶ˆæ¯é¡ºåºä¸€è‡´
    """
    file_upload_ids = []

    for message in messages:
        if not message.photo:
            continue

        # Telegram è¿”å›å¤šä¸ªåˆ†è¾¨ç‡çš„å›¾ç‰‡ï¼Œæœ€åä¸€ä¸ªæ˜¯æœ€é«˜åˆ†è¾¨ç‡
        photo = message.photo[-1]

        try:
            # ä¸‹è½½å›¾ç‰‡
            file = context.bot.get_file(photo.file_id)
            temp_path = os.path.join(tempfile.gettempdir(), f"{photo.file_unique_id}.jpg")
            file.download(temp_path)

            try:
                # ä¸Šä¼ å›¾ç‰‡åˆ° Notion
                logger.info(f"å¼€å§‹ä¸Šä¼ å›¾ç‰‡åˆ° Notion: {temp_path}")
                file_upload_id = upload_image_to_notion(file_path=temp_path)

                if file_upload_id:
                    file_upload_ids.append(file_upload_id)
                    logger.info(f"å›¾ç‰‡ä¸Šä¼ æˆåŠŸï¼Œfile_upload_id: {file_upload_id}")
                else:
                    logger.error("å›¾ç‰‡ä¸Šä¼ åˆ° Notion å¤±è´¥")

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(temp_path)
                except OSError as file_error:
                    logger.warning(f"æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼š{file_error}")

        except Exception as e:
            logger.error(f"ä¸‹è½½æˆ–ä¸Šä¼ å›¾ç‰‡æ—¶å‡ºé”™ï¼š{e}")

    return file_upload_ids


def process_message(update: Update, context: CallbackContext) -> None:
    """å¤„ç†æ”¶åˆ°çš„æ¶ˆæ¯"""
    if update.effective_user.id not in ALLOWED_USER_IDS:
        return

    message = update.message

    # æ£€æµ‹æ˜¯å¦æ˜¯ media groupï¼ˆå¤šå›¾æ¶ˆæ¯ï¼‰
    if message.media_group_id and message.photo:
        from ..media_group import get_collector
        collector = get_collector()
        if collector and collector.add_message(update, context):
            # æ¶ˆæ¯å·²åŠ å…¥æ”¶é›†å™¨ï¼Œç­‰å¾…ç»Ÿä¸€å¤„ç†
            logger.info(
                f"æ¶ˆæ¯ {message.message_id} åŠ å…¥ media group {message.media_group_id}"
            )
            return

    text = None
    entities = None
    contains_photo = message.photo and len(message.photo) > 0

    # è·å–æ–‡æœ¬å†…å®¹å’Œå®ä½“ï¼ŒåŒºåˆ†æ™®é€šæ–‡æœ¬å’Œå¸¦æ ‡é¢˜çš„åª’ä½“æ¶ˆæ¯
    if message.text:
        text = message.text
        entities = message.entities
    elif message.caption:
        text = message.caption
        entities = message.caption_entities
    else:
        text = ""
        entities = []

    # å¤„ç†æ¶ˆæ¯å®ä½“ï¼Œæå–æ ¼å¼åŒ–ä¿¡æ¯
    parsed_content = parse_message_entities(text, entities)

    # ä»åŸå§‹æ–‡æœ¬ä¸­æå– hashtag æ ‡ç­¾
    original_hashtags = extract_hashtags(parsed_content["text"])
    
    # ä»æ–‡æœ¬ä¸­ç§»é™¤ hashtag æ ‡ç­¾ï¼Œå¾—åˆ°ç”¨äºåˆ†æçš„æ¸…æ´æ–‡æœ¬
    cleaned_text = remove_hashtags_from_text(parsed_content["text"])
    
    # å¦‚æœç§»é™¤æ ‡ç­¾åæ–‡æœ¬ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œä¿ç•™åŸæ–‡æœ¬è¿›è¡Œå¤„ç†
    if not cleaned_text.strip() or len(cleaned_text.strip()) < 10:
        content_for_analysis = parsed_content["text"]
        content_for_storage = parsed_content["text"]
    else:
        content_for_analysis = cleaned_text
        content_for_storage = parsed_content["text"]  # ä¿å­˜æ—¶ä½¿ç”¨åŸå§‹æ–‡æœ¬ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰

    # è·å–åˆ›å»ºæ—¶é—´
    created_at = message.date

    # å¦‚æœæ¶ˆæ¯åŒ…å«å›¾ç‰‡ï¼Œæ·»åŠ å‰ç¼€
    if contains_photo:
        logger.info(
            f"æ¥æ”¶åˆ°åŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯ï¼Œç”¨æˆ· ID: {update.effective_user.id}ï¼Œå°†å¤„ç†å›¾ç‰‡å’Œæ–‡å­—"
        )

        # å¦‚æœå›¾ç‰‡æ¶ˆæ¯æ²¡æœ‰æ–‡å­—è¯´æ˜ï¼Œä½¿ç”¨é»˜è®¤è¯´æ˜
        if not text:
            text = "å›¾ç‰‡æ¶ˆæ¯"
            content_for_storage = "å›¾ç‰‡æ¶ˆæ¯"
            content_for_analysis = "ç”¨æˆ·åˆ†äº«çš„å›¾ç‰‡"
        else:
            # ç»™åŸå§‹å†…å®¹æ·»åŠ å‰ç¼€ï¼Œè¡¨æ˜å®ƒæ¥è‡ªåŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯
            content_for_storage = f"[æ­¤å†…å®¹æ¥è‡ªåŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯] {content_for_storage}"

        logger.info(f"å¤„ç†å›¾ç‰‡æ¶ˆæ¯çš„æ–‡å­—å†…å®¹ï¼Œé•¿åº¦ï¼š{len(content_for_storage)} å­—ç¬¦")

    # æå–æ‰€æœ‰ URLï¼ˆä»å®ä½“å’Œæ–‡æœ¬ï¼‰
    urls = extract_urls_from_entities(text, entities)

    # æ£€æŸ¥ç‰¹æ®Šæ ‡ç­¾ï¼ˆä»åŸå§‹æ ‡ç­¾ä¸­æ£€æŸ¥ï¼‰
    if "test" in original_hashtags:
        handle_test_message(update, parsed_content)
        return

    if "todo" in original_hashtags:
        handle_todo_message(update, content_for_storage, created_at)
        return

    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯ URL æ¶ˆæ¯ï¼ˆä½¿ç”¨æ¸…æ´æ–‡æœ¬æ£€æŸ¥ï¼‰
    if urls and is_url_only(cleaned_text if cleaned_text.strip() else content_for_storage):
        handle_url_message(update, urls[0], created_at)
        return

    # å¤š URL å¤„ç†
    if len(urls) > 1:
        handle_multiple_urls_message(update, content_for_storage, urls, created_at)
        return
    elif len(urls) == 1:
        url = urls[0]
    else:
        url = ""

    # çŸ­å†…å®¹å¤„ç†ï¼šå¦‚æœå†…å®¹ä¸æ˜¯çº¯ URL ä¸”å°‘äº 200 å­—ç¬¦ï¼Œç›´æ¥å°†å†…å®¹ä½œä¸ºæ‘˜è¦
    if len(content_for_analysis) < 200:
        # é€šçŸ¥ç”¨æˆ·æ­£åœ¨å¤„ç†æ¶ˆæ¯
        processing_msg = (
            "æ­£åœ¨å¤„ç†æ¶ˆæ¯..." if not contains_photo else "æ­£åœ¨å¤„ç†å›¾ç‰‡æ¶ˆæ¯..."
        )
        update.message.reply_text(processing_msg, parse_mode=None)  # ç¦ç”¨ Markdown è§£æ

        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œå…ˆä¸Šä¼ å›¾ç‰‡
        file_upload_ids = []
        if contains_photo:
            file_upload_ids = download_and_upload_photos(message, context)

        # ä»éœ€ä½¿ç”¨ Gemini API åˆ†ææå–æ ‡ç­¾
        analysis_result = analyze_content(content_for_analysis)

        # åˆå¹¶åŸå§‹æ ‡ç­¾å’Œ AI æ ‡ç­¾
        merged_tags = merge_tags(original_hashtags, analysis_result["tags"])

        # å­˜å…¥ Notionï¼Œä½†ä½¿ç”¨åŸå§‹å†…å®¹ä½œä¸ºæ‘˜è¦
        try:
            from services.notion_service import add_to_notion

            result = add_to_notion(
                content=content_for_storage,  # ä¿å­˜åŒ…å«æ ‡ç­¾çš„åŸå§‹å†…å®¹
                summary=cleaned_text if cleaned_text.strip() else content_for_storage,  # æ‘˜è¦ä½¿ç”¨æ¸…æ´æ–‡æœ¬
                tags=merged_tags,  # ä½¿ç”¨åˆå¹¶åçš„æ ‡ç­¾
                url=url,
                created_at=created_at,
                file_upload_ids=file_upload_ids if file_upload_ids else None,
            )

            # æ„å»ºå›å¤æ¶ˆæ¯
            reply_parts = ["âœ… å·²ä¿å­˜åˆ° Notion"]
            if file_upload_ids:
                reply_parts.append(f"ğŸ“· å·²ä¸Šä¼  {len(file_upload_ids)} å¼ å›¾ç‰‡")
            reply_parts.append(f"ğŸ“„ {result['title']}")
            reply_parts.append(f"ğŸ”— {result['url']}")

            update.message.reply_text(
                "\n".join(reply_parts),
                parse_mode=None
            )
        except Exception as e:
            logger.error(f"æ·»åŠ åˆ° Notion æ—¶å‡ºé”™ï¼š{e}")
            update.message.reply_text(
                f"âš ï¸ ä¿å­˜åˆ° Notion æ—¶å‡ºé”™ï¼š{str(e)}",
                parse_mode=None,  # ç¦ç”¨ Markdown è§£æ
            )
        return

    # é•¿å†…å®¹å¤„ç†ï¼šé€šçŸ¥ç”¨æˆ·æ­£åœ¨å¤„ç†
    processing_msg = (
        "æ­£åœ¨å¤„ç†è¾ƒé•¿æ¶ˆæ¯ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´..."
        if not contains_photo
        else "æ­£åœ¨å¤„ç†å›¾ç‰‡æ¶ˆæ¯ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´..."
    )
    update.message.reply_text(processing_msg, parse_mode=None)  # ç¦ç”¨ Markdown è§£æ

    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œå…ˆä¸Šä¼ å›¾ç‰‡
    file_upload_ids = []
    if contains_photo:
        file_upload_ids = download_and_upload_photos(message, context)

    # ä½¿ç”¨ Gemini API å®Œæ•´åˆ†æå†…å®¹ï¼ˆä½¿ç”¨æ¸…æ´æ–‡æœ¬ï¼‰
    analysis_result = analyze_content(content_for_analysis)

    # åˆå¹¶åŸå§‹æ ‡ç­¾å’Œ AI æ ‡ç­¾
    merged_tags = merge_tags(original_hashtags, analysis_result["tags"])

    # å­˜å…¥ Notion
    try:
        from services.notion_service import add_to_notion

        result = add_to_notion(
            content=content_for_storage,  # ä¿å­˜åŒ…å«æ ‡ç­¾çš„åŸå§‹å†…å®¹
            summary=analysis_result["summary"],
            tags=merged_tags,  # ä½¿ç”¨åˆå¹¶åçš„æ ‡ç­¾
            url=url,
            created_at=created_at,
            file_upload_ids=file_upload_ids if file_upload_ids else None,
        )

        # æ„å»ºå›å¤æ¶ˆæ¯
        reply_parts = ["âœ… å·²ä¿å­˜åˆ° Notion"]
        if file_upload_ids:
            reply_parts.append(f"ğŸ“· å·²ä¸Šä¼  {len(file_upload_ids)} å¼ å›¾ç‰‡")
        reply_parts.append(f"ğŸ“„ {result['title']}")
        reply_parts.append(f"ğŸ”— {result['url']}")

        update.message.reply_text(
            "\n".join(reply_parts),
            parse_mode=None
        )
    except Exception as e:
        logger.error(f"æ·»åŠ åˆ° Notion æ—¶å‡ºé”™ï¼š{e}")
        update.message.reply_text(
            f"âš ï¸ ä¿å­˜åˆ° Notion æ—¶å‡ºé”™ï¼š{str(e)}",
            parse_mode=None,  # ç¦ç”¨ Markdown è§£æ
        )


def process_document(update: Update, context: CallbackContext) -> None:
    """å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œç‰¹åˆ«æ˜¯ PDF"""
    if update.effective_user.id not in ALLOWED_USER_IDS:
        return

    message = update.message

    # æ£€æŸ¥æ˜¯å¦æ˜¯ PDF æ–‡ä»¶
    if message.document.file_name.lower().endswith(".pdf"):
        handle_pdf_document(update, context)
    else:
        # å¯¹äºé PDF æ–‡ä»¶ï¼Œä½¿ç”¨å¸¸è§„å¤„ç†
        process_message(update, context)


def process_media_group(messages: List[Message], update: Update, context: CallbackContext) -> None:
    """
    å¤„ç† media groupï¼ˆå¤šå›¾æ¶ˆæ¯ï¼‰

    å‚æ•°ï¼š
        messages: å±äºåŒä¸€ media group çš„æ‰€æœ‰æ¶ˆæ¯åˆ—è¡¨ï¼Œå·²æŒ‰ message_id æ’åº
        update: ç¬¬ä¸€æ¡æ¶ˆæ¯çš„ update å¯¹è±¡ï¼ˆç”¨äºå›å¤ï¼‰
        context: Telegram ä¸Šä¸‹æ–‡å¯¹è±¡
    """
    if not messages:
        logger.warning("process_media_group æ”¶åˆ°ç©ºæ¶ˆæ¯åˆ—è¡¨")
        return

    logger.info(f"å¼€å§‹å¤„ç† media groupï¼Œå…± {len(messages)} å¼ å›¾ç‰‡")

    # è·å–ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆé€šå¸¸åŒ…å« captionï¼‰
    first_message = messages[0]

    # æå–æ–‡æœ¬å†…å®¹ï¼ˆcaption é€šå¸¸åªåœ¨ç¬¬ä¸€æ¡æ¶ˆæ¯ä¸­ï¼‰
    text = ""
    entities = []
    for msg in messages:
        if msg.caption:
            text = msg.caption
            entities = msg.caption_entities or []
            break

    # è·å–åˆ›å»ºæ—¶é—´
    created_at = first_message.date

    # é€šçŸ¥ç”¨æˆ·æ­£åœ¨å¤„ç†
    try:
        context.bot.send_message(
            chat_id=first_message.chat_id,
            text=f"æ­£åœ¨å¤„ç† {len(messages)} å¼ å›¾ç‰‡...",
            reply_to_message_id=first_message.message_id,
            parse_mode=None
        )
    except Exception as e:
        logger.warning(f"å‘é€å¤„ç†é€šçŸ¥å¤±è´¥: {e}")

    # æ‰¹é‡ä¸Šä¼ æ‰€æœ‰å›¾ç‰‡
    file_upload_ids = download_and_upload_photos_batch(messages, context)
    logger.info(f"æˆåŠŸä¸Šä¼  {len(file_upload_ids)} å¼ å›¾ç‰‡")

    # å¤„ç†æ–‡æœ¬å†…å®¹
    if text:
        parsed_content = parse_message_entities(text, entities)
        original_hashtags = extract_hashtags(parsed_content["text"])
        cleaned_text = remove_hashtags_from_text(parsed_content["text"])

        if not cleaned_text.strip() or len(cleaned_text.strip()) < 10:
            content_for_analysis = parsed_content["text"]
            content_for_storage = parsed_content["text"]
        else:
            content_for_analysis = cleaned_text
            content_for_storage = parsed_content["text"]

        # ç»™åŸå§‹å†…å®¹æ·»åŠ å‰ç¼€
        content_for_storage = f"[æ­¤å†…å®¹æ¥è‡ªåŒ…å« {len(messages)} å¼ å›¾ç‰‡çš„æ¶ˆæ¯] {content_for_storage}"
    else:
        # æ²¡æœ‰æ–‡å­—è¯´æ˜
        content_for_storage = f"å›¾ç‰‡æ¶ˆæ¯ï¼ˆ{len(messages)} å¼ å›¾ç‰‡ï¼‰"
        content_for_analysis = "ç”¨æˆ·åˆ†äº«çš„å¤šå¼ å›¾ç‰‡"
        original_hashtags = []
        cleaned_text = ""

    # æå– URL
    urls = extract_urls_from_entities(text, entities) if text else []
    url = urls[0] if urls else ""

    # ä½¿ç”¨ Gemini API åˆ†æå†…å®¹
    analysis_result = analyze_content(content_for_analysis)

    # åˆå¹¶åŸå§‹æ ‡ç­¾å’Œ AI æ ‡ç­¾
    merged_tags = merge_tags(original_hashtags, analysis_result["tags"])

    # å­˜å…¥ Notion
    try:
        from services.notion_service import add_to_notion

        result = add_to_notion(
            content=content_for_storage,
            summary=cleaned_text if cleaned_text.strip() else content_for_storage,
            tags=merged_tags,
            url=url,
            created_at=created_at,
            file_upload_ids=file_upload_ids if file_upload_ids else None,
        )

        # æ„å»ºå›å¤æ¶ˆæ¯
        reply_parts = ["âœ… å·²ä¿å­˜åˆ° Notion"]
        reply_parts.append(f"ğŸ“· å·²ä¸Šä¼  {len(file_upload_ids)} å¼ å›¾ç‰‡")
        reply_parts.append(f"ğŸ“„ {result['title']}")
        reply_parts.append(f"ğŸ”— {result['url']}")

        context.bot.send_message(
            chat_id=first_message.chat_id,
            text="\n".join(reply_parts),
            reply_to_message_id=first_message.message_id,
            parse_mode=None
        )
    except Exception as e:
        logger.error(f"æ·»åŠ åˆ° Notion æ—¶å‡ºé”™ï¼š{e}")
        context.bot.send_message(
            chat_id=first_message.chat_id,
            text=f"âš ï¸ ä¿å­˜åˆ° Notion æ—¶å‡ºé”™ï¼š{str(e)}",
            reply_to_message_id=first_message.message_id,
            parse_mode=None
        )
